{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJVRsvOZRqtRv3V+LjGnh3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sravani-05/Assignment03/blob/main/297_JAX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random, grad, jit\n",
        "from flax import linen as nn\n",
        "import optax"
      ],
      "metadata": {
        "id": "0K8exYREDAYt"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "batch_size = 16\n",
        "block_size = 32\n",
        "learning_rate = 1e-3\n",
        "max_iters = 1000\n",
        "n_embd = 64\n",
        "vocab_size = 256  # Assuming ASCII\n",
        "\n",
        "rng_key = random.PRNGKey(0)"
      ],
      "metadata": {
        "id": "iCiZdy1jDCgp"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        x = x[..., None]  # Adding an embedding dimension\n",
        "        x = nn.Dense(n_embd)(x)\n",
        "        x = nn.LayerNorm()(x)\n",
        "        x = nn.SelfAttention(num_heads=2)(x)\n",
        "        x = x.reshape((x.shape[0], x.shape[1], -1))  # Flattening the last dimensions\n",
        "        x = nn.Dense(vocab_size)(x)\n",
        "        return x\n",
        "\n",
        "@jit\n",
        "def softmax_cross_entropy(logits, targets):\n",
        "    logits_reshaped = logits.reshape((-1, vocab_size))\n",
        "    targets_reshaped = targets.reshape((-1,))\n",
        "    logprobs = jax.nn.log_softmax(logits_reshaped)\n",
        "\n",
        "    targets_one_hot = jax.nn.one_hot(targets_reshaped, vocab_size)\n",
        "\n",
        "    # Element-wise multiplication and sum over the vocab_size dimension\n",
        "    loss_values = -jnp.sum(targets_one_hot * logprobs, axis=-1)\n",
        "\n",
        "    # Reshape loss values back to (batch_size, block_size)\n",
        "    return loss_values.reshape((batch_size, block_size))\n",
        "\n",
        "@jit\n",
        "def compute_loss(params, x, y):\n",
        "    logits = model.apply(params, x)\n",
        "    loss_values = softmax_cross_entropy(logits, y)\n",
        "    mean_loss = jnp.mean(loss_values)\n",
        "    return mean_loss\n",
        "\n",
        "@jit\n",
        "def update(params, x, y, opt_state):\n",
        "    opt_update = optimizer.update\n",
        "    loss, grads = jax.value_and_grad(compute_loss)(params, x, y)  # removed `model` from the arguments\n",
        "    updates, new_opt_state = opt_update(grads, opt_state)\n",
        "    new_params = optax.apply_updates(params, updates)\n",
        "    return new_params, new_opt_state, loss\n",
        "\n",
        "\n",
        "# Data (for demonstration purposes, use real data in practice)\n",
        "data = jnp.array([i % vocab_size for i in range(10000)], dtype=jnp.int32)\n",
        "def get_batch():\n",
        "    idx = random.randint(rng_key, (batch_size,), 0, len(data) - block_size - 1)\n",
        "    x = jnp.array([data[i:i+block_size] for i in idx])\n",
        "    y = jnp.array([data[i+1:i+block_size+1] for i in idx])\n",
        "    return x, y\n",
        "\n",
        "# Training\n",
        "model = Transformer()\n",
        "params = model.init(rng_key, jnp.ones((batch_size, block_size)))\n",
        "optimizer = optax.adam(learning_rate)\n",
        "opt_state = optimizer.init(params)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    x, y = get_batch()\n",
        "    params, opt_state, loss = update(params, x, y, opt_state)\n",
        "    if iter % 100 == 0:\n",
        "        print(f\"Iteration {iter}, Loss: {loss}\")\n",
        "\n",
        "# Additional utility function to convert a string to its ASCII representation\n",
        "def string_to_ascii(input_str):\n",
        "    return jnp.array([ord(c) for c in input_str], dtype=jnp.int32)\n",
        "\n",
        "# Simple text generation\n",
        "def generate_text(params, model, start_token=0, length=100):\n",
        "    generated = [start_token]\n",
        "\n",
        "    # Initialize a sequence of length `block_size` filled with the `start_token`\n",
        "    current_token = jnp.array([start_token] * block_size).reshape(1, block_size)\n",
        "\n",
        "    for _ in range(length):\n",
        "        logits = model.apply(params, current_token)  # Generate logits for the sequence\n",
        "        next_token = jnp.argmax(logits[0, -1])\n",
        "        generated.append(int(next_token))\n",
        "\n",
        "        # Append the next_token to current_token sequence and use only the last `block_size` tokens\n",
        "        current_token = jnp.concatenate([current_token, next_token.reshape(1, 1)], axis=1)[:, -block_size:]\n",
        "\n",
        "    return generated\n",
        "\n",
        "# Initialize the model with a dummy input that matches the shape of our generation process\n",
        "dummy_input = jnp.ones((1, block_size))\n",
        "params_gen = model.init(rng_key, dummy_input)\n",
        "\n",
        "# Update the params with trained weights\n",
        "params_gen = params\n",
        "\n",
        "def generate_text(params, model, start_string, length=100):\n",
        "    start_tokens = string_to_ascii(start_string)\n",
        "    generated = list(start_tokens)\n",
        "\n",
        "    # If the initial tokens are fewer than block_size, pad them\n",
        "    if len(start_tokens) < block_size:\n",
        "        current_token = jnp.pad(start_tokens, (block_size - len(start_tokens), 0), mode='constant')\n",
        "    else:\n",
        "        current_token = start_tokens[-block_size:]  # Take the last `block_size` characters\n",
        "\n",
        "    current_token = current_token.reshape(1, block_size)\n",
        "\n",
        "    for _ in range(length):\n",
        "        logits = model.apply(params, current_token)\n",
        "        next_token = jnp.argmax(logits[0, -1])\n",
        "        generated.append(int(next_token))\n",
        "\n",
        "        # Use the most recent `block_size` tokens for the next step\n",
        "        current_token = jnp.concatenate([current_token, next_token.reshape(1, 1)], axis=1)[:, -block_size:]\n",
        "\n",
        "    return \"\".join([chr(c) for c in generated])\n",
        "\n",
        "print(generate_text(params_gen, model, start_string=\"once upon a time\", length=100))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrOV9_XADEdN",
        "outputId": "0b2d9222-30d0-4127-8d1b-25ec028d0732"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, Loss: 6.21189546585083\n",
            "Iteration 100, Loss: 4.883039474487305\n",
            "Iteration 200, Loss: 4.875761985778809\n",
            "Iteration 300, Loss: 4.875408172607422\n",
            "Iteration 400, Loss: 4.875296592712402\n",
            "Iteration 500, Loss: 4.875243186950684\n",
            "Iteration 600, Loss: 4.875209331512451\n",
            "Iteration 700, Loss: 4.875184535980225\n",
            "Iteration 800, Loss: 4.875168800354004\n",
            "Iteration 900, Loss: 4.875140190124512\n",
            "once upon a timeõõõõõõõõõõõõõõõö\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "batch_size = 16\n",
        "block_size = 32\n",
        "learning_rate = 1e-3\n",
        "max_iters = 4000\n",
        "n_embd = 64\n",
        "vocab_size = 256  # Assuming ASCII\n",
        "\n",
        "rng_key = random.PRNGKey(0)"
      ],
      "metadata": {
        "id": "7iOkLXI0DZ-c"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        x = x[..., None]  # Adding an embedding dimension\n",
        "        x = nn.Dense(n_embd)(x)\n",
        "        x = nn.LayerNorm()(x)\n",
        "        x = nn.SelfAttention(num_heads=2)(x)\n",
        "        x = x.reshape((x.shape[0], x.shape[1], -1))  # Flattening the last dimensions\n",
        "        x = nn.Dense(vocab_size)(x)\n",
        "        return x\n",
        "\n",
        "@jit\n",
        "def softmax_cross_entropy(logits, targets):\n",
        "    logits_reshaped = logits.reshape((-1, vocab_size))\n",
        "    targets_reshaped = targets.reshape((-1,))\n",
        "    logprobs = jax.nn.log_softmax(logits_reshaped)\n",
        "\n",
        "    targets_one_hot = jax.nn.one_hot(targets_reshaped, vocab_size)\n",
        "\n",
        "    # Element-wise multiplication and sum over the vocab_size dimension\n",
        "    loss_values = -jnp.sum(targets_one_hot * logprobs, axis=-1)\n",
        "\n",
        "    # Reshape loss values back to (batch_size, block_size)\n",
        "    return loss_values.reshape((batch_size, block_size))\n",
        "\n",
        "@jit\n",
        "def compute_loss(params, x, y):\n",
        "    logits = model.apply(params, x)\n",
        "    loss_values = softmax_cross_entropy(logits, y)\n",
        "    mean_loss = jnp.mean(loss_values)\n",
        "    return mean_loss\n",
        "\n",
        "@jit\n",
        "def update(params, x, y, opt_state):\n",
        "    opt_update = optimizer.update\n",
        "    loss, grads = jax.value_and_grad(compute_loss)(params, x, y)  # removed `model` from the arguments\n",
        "    updates, new_opt_state = opt_update(grads, opt_state)\n",
        "    new_params = optax.apply_updates(params, updates)\n",
        "    return new_params, new_opt_state, loss\n",
        "\n",
        "\n",
        "# Data (for demonstration purposes, use real data in practice)\n",
        "data = jnp.array([i % vocab_size for i in range(10000)], dtype=jnp.int32)\n",
        "def get_batch():\n",
        "    idx = random.randint(rng_key, (batch_size,), 0, len(data) - block_size - 1)\n",
        "    x = jnp.array([data[i:i+block_size] for i in idx])\n",
        "    y = jnp.array([data[i+1:i+block_size+1] for i in idx])\n",
        "    return x, y\n",
        "\n",
        "# Training\n",
        "model = Transformer()\n",
        "params = model.init(rng_key, jnp.ones((batch_size, block_size)))\n",
        "optimizer = optax.adam(learning_rate)\n",
        "opt_state = optimizer.init(params)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    x, y = get_batch()\n",
        "    params, opt_state, loss = update(params, x, y, opt_state)\n",
        "    if iter % 100 == 0:\n",
        "        print(f\"Iteration {iter}, Loss: {loss}\")\n",
        "\n",
        "# Additional utility function to convert a string to its ASCII representation\n",
        "def string_to_ascii(input_str):\n",
        "    return jnp.array([ord(c) for c in input_str], dtype=jnp.int32)\n",
        "\n",
        "# Simple text generation\n",
        "def generate_text(params, model, start_token=0, length=100):\n",
        "    generated = [start_token]\n",
        "\n",
        "    # Initialize a sequence of length `block_size` filled with the `start_token`\n",
        "    current_token = jnp.array([start_token] * block_size).reshape(1, block_size)\n",
        "\n",
        "    for _ in range(length):\n",
        "        logits = model.apply(params, current_token)  # Generate logits for the sequence\n",
        "        next_token = jnp.argmax(logits[0, -1])\n",
        "        generated.append(int(next_token))\n",
        "\n",
        "        # Append the next_token to current_token sequence and use only the last `block_size` tokens\n",
        "        current_token = jnp.concatenate([current_token, next_token.reshape(1, 1)], axis=1)[:, -block_size:]\n",
        "\n",
        "    return generated\n",
        "\n",
        "# Initialize the model with a dummy input that matches the shape of our generation process\n",
        "dummy_input = jnp.ones((1, block_size))\n",
        "params_gen = model.init(rng_key, dummy_input)\n",
        "\n",
        "# Update the params with trained weights\n",
        "params_gen = params\n",
        "\n",
        "def generate_text(params, model, start_string, length=100):\n",
        "    start_tokens = string_to_ascii(start_string)\n",
        "    generated = list(start_tokens)\n",
        "\n",
        "    # If the initial tokens are fewer than block_size, pad them\n",
        "    if len(start_tokens) < block_size:\n",
        "        current_token = jnp.pad(start_tokens, (block_size - len(start_tokens), 0), mode='constant')\n",
        "    else:\n",
        "        current_token = start_tokens[-block_size:]  # Take the last `block_size` characters\n",
        "\n",
        "    current_token = current_token.reshape(1, block_size)\n",
        "\n",
        "    for _ in range(length):\n",
        "        logits = model.apply(params, current_token)\n",
        "        next_token = jnp.argmax(logits[0, -1])\n",
        "        generated.append(int(next_token))\n",
        "\n",
        "        # Use the most recent `block_size` tokens for the next step\n",
        "        current_token = jnp.concatenate([current_token, next_token.reshape(1, 1)], axis=1)[:, -block_size:]\n",
        "\n",
        "    return \"\".join([chr(c) for c in generated])\n",
        "\n",
        "print(generate_text(params_gen, model, start_string=\"\"\"O Captain! My Captain! our fearful trip is done;\n",
        "The ship has weather'd every rack, the prize we sought is won;\n",
        "The port is near, the bells I hear, the people all exulting,\n",
        "While follow eyes the steady keel, the vessel grim and daring:\n",
        "\n",
        "But O heart! heart! heart!\n",
        "O the bleeding drops of red,\n",
        "Where on the deck my Captain lies,\n",
        "Fallen cold and dead.\n",
        "\n",
        "O Captain! my Captain! rise up and hear the bells;\n",
        "Rise up—for you the flag is flung—for you the bugle trills;\n",
        "For you bouquets and ribbon'd wreaths—for you the shores a-crowding;\n",
        "For you they call, the swaying mass, their eager faces turning;\n",
        "\n",
        "O captain! dear father!\n",
        "This arm beneath your head;\n",
        "It is some dream that on the deck,\n",
        "You've fallen cold and dead.\n",
        "\n",
        "My Captain does not answer, his lips are pale and still;\n",
        "My father does not feel my arm, he has no pulse nor will;\n",
        "The ship is anchor'd safe and sound, its voyage closed and done\"\"\", length=100))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etEtF7C7De0F",
        "outputId": "b3e1a3d3-8028-408d-f2fc-f8493320ad2a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, Loss: 6.21189546585083\n",
            "Iteration 100, Loss: 4.883039474487305\n",
            "Iteration 200, Loss: 4.875761985778809\n",
            "Iteration 300, Loss: 4.875408172607422\n",
            "Iteration 400, Loss: 4.875296592712402\n",
            "Iteration 500, Loss: 4.875243186950684\n",
            "Iteration 600, Loss: 4.875209331512451\n",
            "Iteration 700, Loss: 4.875184535980225\n",
            "Iteration 800, Loss: 4.875168800354004\n",
            "Iteration 900, Loss: 4.875140190124512\n",
            "Iteration 1000, Loss: 4.875114440917969\n",
            "Iteration 1100, Loss: 4.875144958496094\n",
            "Iteration 1200, Loss: 4.875063419342041\n",
            "Iteration 1300, Loss: 4.874737739562988\n",
            "Iteration 1400, Loss: 4.875070571899414\n",
            "Iteration 1500, Loss: 4.875022888183594\n",
            "Iteration 1600, Loss: 4.874975204467773\n",
            "Iteration 1700, Loss: 4.87492561340332\n",
            "Iteration 1800, Loss: 4.874856948852539\n",
            "Iteration 1900, Loss: 4.87477970123291\n",
            "Iteration 2000, Loss: 4.874793529510498\n",
            "Iteration 2100, Loss: 4.874538421630859\n",
            "Iteration 2200, Loss: 4.87564754486084\n",
            "Iteration 2300, Loss: 4.874076843261719\n",
            "Iteration 2400, Loss: 4.873717308044434\n",
            "Iteration 2500, Loss: 4.873130798339844\n",
            "Iteration 2600, Loss: 4.872415542602539\n",
            "Iteration 2700, Loss: 4.870602607727051\n",
            "Iteration 2800, Loss: 4.868008136749268\n",
            "Iteration 2900, Loss: 4.862888336181641\n",
            "Iteration 3000, Loss: 4.847123146057129\n",
            "Iteration 3100, Loss: 4.814708709716797\n",
            "Iteration 3200, Loss: 4.708573341369629\n",
            "Iteration 3300, Loss: 5.212739944458008\n",
            "Iteration 3400, Loss: 4.592184066772461\n",
            "Iteration 3500, Loss: 4.520346641540527\n",
            "Iteration 3600, Loss: 4.438632011413574\n",
            "Iteration 3700, Loss: 4.371338367462158\n",
            "Iteration 3800, Loss: 4.316313743591309\n",
            "Iteration 3900, Loss: 4.344044208526611\n",
            "O Captain! My Captain! our fearful trip is done;\n",
            "The ship has weather'd every rack, the prize we sought is won;\n",
            "The port is near, the bells I hear, the people all exulting,\n",
            "While follow eyes the steady keel, the vessel grim and daring:\n",
            "\n",
            "But O heart! heart! heart!\n",
            "O the bleeding drops of red,\n",
            "Where on the deck my Captain lies,\n",
            "Fallen cold and dead.\n",
            "\n",
            "O Captain! my Captain! rise up and hear the bells;\n",
            "Rise up—for you the flag is flung—for you the bugle trills;\n",
            "For you bouquets and ribbon'd wreaths—for you the shores a-crowding;\n",
            "For you they call, the swaying mass, their eager faces turning;\n",
            "\n",
            "O captain! dear father!\n",
            "This arm beneath your head;\n",
            "It is some dream that on the deck,\n",
            "You've fallen cold and dead.\n",
            "\n",
            "My Captain does not answer, his lips are pale and still;\n",
            "My father does not feel my arm, he has no pulse nor will;\n",
            "The ship is anchor'd safe and sound, its voyage closed and doneBBBBBBBBBBBBBBBBBBBBBBBBBBBB>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "from jax import random, grad, jit\n",
        "from flax import linen as nn\n",
        "import optax\n",
        "\n",
        "# Parameters\n",
        "batch_size = 16\n",
        "block_size = 32\n",
        "learning_rate = 1e-3\n",
        "max_iters = 1000\n",
        "n_embd = 64\n",
        "vocab_size = 256  # Assuming ASCII\n",
        "\n",
        "rng_key = random.PRNGKey(0)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        x = x[..., None]  # Adding an embedding dimension\n",
        "        x = nn.Dense(n_embd)(x)\n",
        "        x = nn.LayerNorm()(x)\n",
        "        x = nn.SelfAttention(num_heads=2)(x)\n",
        "        x = x.reshape((x.shape[0], x.shape[1], -1))  # Flattening the last dimensions\n",
        "        x = nn.Dense(vocab_size)(x)\n",
        "        return x\n",
        "\n",
        "@jit\n",
        "def softmax_cross_entropy(logits, targets):\n",
        "    logits_reshaped = logits.reshape((-1, vocab_size))\n",
        "    targets_reshaped = targets.reshape((-1,))\n",
        "    logprobs = jax.nn.log_softmax(logits_reshaped)\n",
        "\n",
        "    targets_one_hot = jax.nn.one_hot(targets_reshaped, vocab_size)\n",
        "\n",
        "    # Element-wise multiplication and sum over the vocab_size dimension\n",
        "    loss_values = -jnp.sum(targets_one_hot * logprobs, axis=-1)\n",
        "\n",
        "    # Reshape loss values back to (batch_size, block_size)\n",
        "    return loss_values.reshape((batch_size, block_size))\n",
        "\n",
        "@jit\n",
        "def compute_loss(params, x, y):\n",
        "    logits = model.apply(params, x)\n",
        "    loss_values = softmax_cross_entropy(logits, y)\n",
        "    mean_loss = jnp.mean(loss_values)\n",
        "    return mean_loss\n",
        "\n",
        "@jit\n",
        "def update(params, x, y, opt_state):\n",
        "    opt_update = optimizer.update\n",
        "    loss, grads = jax.value_and_grad(compute_loss)(params, x, y)  # removed `model` from the arguments\n",
        "    updates, new_opt_state = opt_update(grads, opt_state)\n",
        "    new_params = optax.apply_updates(params, updates)\n",
        "    return new_params, new_opt_state, loss\n",
        "\n",
        "\n",
        "# Data (for demonstration purposes, use real data in practice)\n",
        "data = jnp.array([i % vocab_size for i in range(10000)], dtype=jnp.int32)\n",
        "def get_batch():\n",
        "    idx = random.randint(rng_key, (batch_size,), 0, len(data) - block_size - 1)\n",
        "    x = jnp.array([data[i:i+block_size] for i in idx])\n",
        "    y = jnp.array([data[i+1:i+block_size+1] for i in idx])\n",
        "    return x, y\n",
        "\n",
        "# Training\n",
        "model = Transformer()\n",
        "params = model.init(rng_key, jnp.ones((batch_size, block_size)))\n",
        "optimizer = optax.adam(learning_rate)\n",
        "opt_state = optimizer.init(params)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    x, y = get_batch()\n",
        "    params, opt_state, loss = update(params, x, y, opt_state)\n",
        "    if iter % 100 == 0:\n",
        "        print(f\"Iteration {iter}, Loss: {loss}\")\n",
        "\n",
        "# Simple text generation\n",
        "def generate_text(params, model, start_token=0, length=100):\n",
        "    generated = [start_token]\n",
        "\n",
        "    # Initialize a sequence of length `block_size` filled with the `start_token`\n",
        "    current_token = jnp.array([start_token] * block_size).reshape(1, block_size)\n",
        "\n",
        "    for _ in range(length):\n",
        "        logits = model.apply(params, current_token)  # Generate logits for the sequence\n",
        "        next_token = jnp.argmax(logits[0, -1])\n",
        "        generated.append(int(next_token))\n",
        "\n",
        "        # Append the next_token to current_token sequence and use only the last `block_size` tokens\n",
        "        current_token = jnp.concatenate([current_token, next_token.reshape(1, 1)], axis=1)[:, -block_size:]\n",
        "\n",
        "    return generated\n",
        "\n",
        "\n",
        "print(generate_text(params, model, start_token=0, length=100))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atama_fAFqLM",
        "outputId": "02e877a9-ae6f-4b61-d9ae-7036c455659c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, Loss: 6.21189546585083\n",
            "Iteration 100, Loss: 4.883039474487305\n",
            "Iteration 200, Loss: 4.875761985778809\n",
            "Iteration 300, Loss: 4.875408172607422\n",
            "Iteration 400, Loss: 4.875296592712402\n",
            "Iteration 500, Loss: 4.875243186950684\n",
            "Iteration 600, Loss: 4.875209331512451\n",
            "Iteration 700, Loss: 4.875184535980225\n",
            "Iteration 800, Loss: 4.875168800354004\n",
            "Iteration 900, Loss: 4.875140190124512\n",
            "[0, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 246, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137]\n"
          ]
        }
      ]
    }
  ]
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sravani-05/Assignment03/blob/main/297_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VunxY8pvlsGv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1ezHZd5l9ZA",
        "outputId": "5ca28e16-ddca-4988-c358-f4c7a8cc6d0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQ_JvMi6mDVl",
        "outputId": "863976df-ac92-4a7e-ea94-e0d04eb948c2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f8bd22cfc30>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.manual_seed(101)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9l19IVj0mrFX"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "block_size = 32\n",
        "max_iters = 5000\n",
        "eval_interval = 1000\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 500\n",
        "n_embd = 64\n",
        "n_head = 8\n",
        "n_layer = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4xi2Bl9axUd"
      },
      "outputs": [],
      "source": [
        "input_file_path = \"/content/drive/MyDrive/Spl Topics/Mission_possible.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtSRKL8GbFwG"
      },
      "outputs": [],
      "source": [
        "class Dataset:\n",
        "  def __init__(self):\n",
        "    self.vocab_size = 0\n",
        "    self.train_data = torch.tensor([])\n",
        "    self.val_data = torch.tensor([])\n",
        "\n",
        "  def read_dataset(self):\n",
        "    with open(input_file_path, 'r', encoding='utf-8') as f:\n",
        "        self.data = f.read()\n",
        "\n",
        "  def prepare_dataset(self):\n",
        "    chars = sorted(list(set(self.data)))\n",
        "    self.vocab_size = len(chars)\n",
        "    char_to_int = { ch:i for i,ch in enumerate(chars) }\n",
        "    int_to_char = { i:ch for i,ch in enumerate(chars) }\n",
        "    self.encode = lambda s: [char_to_int[c] for c in s]\n",
        "    self.decode = lambda l: ''.join([int_to_char[i] for i in l])\n",
        "\n",
        "  def data_split(self):\n",
        "    data_tensor = torch.tensor(self.encode(self.data), dtype=torch.long)\n",
        "    n = int(0.8*len(data_tensor))\n",
        "    self.train_data = data_tensor[:n]\n",
        "    self.val_data = data_tensor[n:]\n",
        "\n",
        "  def get_batch(self, split):\n",
        "    data = self.train_data if split == 'train' else self.val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frYD10eYbIIP"
      },
      "outputs": [],
      "source": [
        "datasetObj = Dataset()\n",
        "datasetObj.read_dataset()\n",
        "datasetObj.prepare_dataset()\n",
        "datasetObj.data_split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0nCvpLrbKN7"
      },
      "outputs": [],
      "source": [
        "class Loss:\n",
        "  @torch.no_grad()\n",
        "  def estimate_loss(self):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = datasetObj.get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wofBnWrbM6O"
      },
      "outputs": [],
      "source": [
        "lossObj = Loss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIoG9quXbOsr"
      },
      "outputs": [],
      "source": [
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.query(x)\n",
        "    w = q @ k.transpose(-2,-1) * C**-0.5\n",
        "    w = w.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "    w = F.softmax(w, dim=-1)\n",
        "\n",
        "    v = self.value(x)\n",
        "    out = w @ v\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HiExnaG6bQtb"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([AttentionHead(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfZbDyV2bS6c"
      },
      "outputs": [],
      "source": [
        "class FeedFoward(nn.Module):\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embd, 4 * n_embd),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4 * n_embd, n_embd))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9U-SfGSbWAI"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, n_embd, n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embd // n_head\n",
        "    self.sa = MultiHeadAttention(n_head, head_size)\n",
        "    self.ffwd = FeedFoward(n_embd)\n",
        "    self.ln1 = nn.LayerNorm(n_embd)\n",
        "    self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.sa(self.ln1(x))\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDuX4UA7bX3z"
      },
      "outputs": [],
      "source": [
        "class NanoGPT(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(datasetObj.vocab_size, n_embd)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    self.blocks = nn.Sequential(*[TransformerBlock(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "    self.ln_f = nn.LayerNorm(n_embd)\n",
        "    self.lm_head = nn.Linear(n_embd, datasetObj.vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "    tok_emb = self.token_embedding_table(idx)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.blocks(x)\n",
        "    x = self.ln_f(x)\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    if targets is None:\n",
        "        loss = None\n",
        "    else:\n",
        "        B, T, C = logits.shape\n",
        "        logits = logits.view(B*T, C)\n",
        "        targets = targets.view(B*T)\n",
        "        loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      logits, loss = self(idx_cond)\n",
        "      logits = logits[:, -1, :]\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDYHLJVVbacp"
      },
      "outputs": [],
      "source": [
        "def generateNext():\n",
        "  context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "  print(datasetObj.decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhMIV8IHbcHf",
        "outputId": "4a60b2ff-c262-4070-dce9-bee42cdfe07f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.406317 M parameters\n",
            "step 0: train loss 3.9828, val loss 3.9749\n",
            "step 1000: train loss 0.1512, val loss 5.0494\n",
            "step 2000: train loss 0.1362, val loss 5.1456\n",
            "step 3000: train loss 0.1381, val loss 5.2695\n",
            "step 4000: train loss 0.1279, val loss 5.4498\n",
            "step 4999: train loss 0.1254, val loss 5.2846\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "  model = NanoGPT()\n",
        "  m = model.to(device)\n",
        "  print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "  optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = lossObj.estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = datasetObj.get_batch('train')\n",
        "\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generateNext()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7dZghyRenFT",
        "outputId": "88e4eae6-2117-4b08-d071-91800b9128a3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " killed when their car is bombed with them inside. His mentor Jim Phelps is killed by an unseen assassin that he had reported was following him on the Charles Bridge. Sarah follows Ethan's order to pursue Golitsyn used to escape. His other teammates Hannah Williams and Claire are killed when their car is bombed with them inside. His mentor Jim Phelps is killed by an unseen assassin that he had reported was following him on the Charles Bridge. Sarah follows Ethan's order to pursue Golitsyn used to escape. His other teammates Hannah Williams and Claire are killed wrong OChim on the Charles Bridge. Sarah follows Ethan's order to pursue Golitsyn used to escape. His other teammates Hannah Williams and Claire are killed when their car is bombed with them inside. His mentor Jim Phelps is killed by an unseen assassin that he had reported was following him on the Charles Bridge. Sarah follows Ethan's order to pursue Golitsyn used to escape. His other teammates Hannah Williams and Claire are killed when their car is bombed with them inside. His mentor Jim Phelps is killed by an unseen assassin that he had reported was following him on the Charles Bridge. when Saridih ndextricabo be fridim ws His ngoals and the team obtains video evidence of Golitsyn stealing the NOC list and exiting the building. Unfortunately, that's when everything goes inextricably wrong. Over their radio frequency, Ethan hears his teammate Jack (Emilio Estevez, uncredited), dying from getting impaled on a spike after losing control of the elevator he had been riding all night, which also happens to be the one Golitsyn used to escape. His other teammates Hannah Williams and Claire are killed when their car is bombed with them inside. His mentor Jim Phelps is killed by an unseen assassin that he had reported was following him on the Charles Bridge. Sarah follows Ethan's order to pursue Golitsyn used to escape. His other teammates Hannah Williams and Claire are rididing are. arah follows Ethan's ororder to pu\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1TQGI4YCjhkXcuIhs6EJJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}